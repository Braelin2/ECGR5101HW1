{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZVCYce4qZWQGixWlyQ68JdBeNsIBwt7C",
      "authorship_tag": "ABX9TyPSGRIuAkxjVqxZ2QaPuXcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Braelin2/ECGR5101HW1/blob/main/Hw1_Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OESKrvLEDQ1P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table\n",
        "from os import MFD_HUGE_256MB\n",
        "df = pd.read_csv(\"D3.csv\")\n",
        "\n",
        "dataX1 = df.values[:,0]\n",
        "dataX2 = df.values[:,1]\n",
        "dataX3 = df.values[:,2]\n",
        "Y = df.values[:,3]\n",
        "\n",
        "m = len(Y)\n",
        "\n",
        "theta0 = np.zeros(2);\n",
        "iterations = 1000\n",
        "alpha = 0.01"
      ],
      "metadata": {
        "id": "_G-x80IZEnxq"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newX0 = np.ones((m,1))\n",
        "newX1 = dataX1.reshape(m, 1)\n",
        "X1 = np.hstack((newX0,newX1))\n",
        "\n",
        "newX2 = dataX2.reshape(m, 1)\n",
        "X2 = np.hstack((newX0,newX2))\n",
        "\n",
        "newX3 = dataX3.reshape(m, 1)\n",
        "X3 = np.hstack((newX0,newX3))"
      ],
      "metadata": {
        "id": "uPG32BmwiZsu"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute cost for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X : 2D array where each row represents the training example and each column represent the feature\n",
        "        m = number of training examples\n",
        "        n = number of features (including X_0 column of ones)\n",
        "    y : 1D array of labels/target values for each training example. dimension(m)\n",
        "    theta : 1D array of fitting parameters or weights. Dimension (n)\n",
        "\n",
        "    Returns:\n",
        "    J : Scalar value, the cost\n",
        "    \"\"\"\n",
        "    predictions = X.dot(theta)\n",
        "    errors = np.subtract(predictions, y)\n",
        "    sqrErrors = np.square(errors)\n",
        "    J = 1 / (2 * m) * np.sum(sqrErrors)\n",
        "    return J"
      ],
      "metadata": {
        "id": "Jk4jzHLNIz8J"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, theta, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Compute the optimal parameters using gradient descent for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X : 2D array where each row represents the training example and each column represents the feature\n",
        "        m = number of training examples\n",
        "        n = number of features (including X_0 column of ones)\n",
        "    y : 1D array of labels/target values for each training example. dimension(m)\n",
        "    theta : 1D array of fitting parameters or weights. Dimension (n)\n",
        "    alpha : Learning rate (scalar)\n",
        "    iterations : Number of iterations (scalar)\n",
        "\n",
        "    Returns:\n",
        "    theta : Updated values of fitting parameters or weights after 'iterations' iterations. Dimension (n)\n",
        "    cost_history : Array containing the cost for each iteration. Dimension (iterations)\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(y)  # Number of training examples\n",
        "    cost_history = np.zeros(iterations)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = X.dot(theta)\n",
        "        errors = predictions - y\n",
        "        theta = theta - (alpha / m) * X.T.dot(errors)\n",
        "        #sum_delta = (alpha / m) * X.transpose().dot(errors)\n",
        "        #theta -= sum_delta\n",
        "        cost_history[i] = compute_cost(X, y, theta)\n",
        "\n",
        "    return theta, cost_history"
      ],
      "metadata": {
        "id": "0w41iyKbHs9s"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta1, cost_history1 = gradient_descent(X1, Y, theta0, alpha, iterations)\n",
        "\n",
        "#print('Final value of theta =', theta1)\n",
        "print('cost_history =', cost_history1)\n",
        "\n",
        "plt.scatter(X1[:,1], Y, color='red', marker='+', label='Training Data')\n",
        "plt.plot(X1[:,1], X1.dot(theta1), color='green', label='Linear Regression')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(f\"X1 vs Y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, iterations + 1), cost_history1, color='blue')\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost (J)')\n",
        "plt.title('Convergence of gradient descent X1 vs Y')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XbSnGMAWKEOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta2, cost_history2 = gradient_descent(X2, Y, theta0, alpha, iterations)\n",
        "\n",
        "#print('Final value of theta =', theta2)\n",
        "print('cost_history =', cost_history2)\n",
        "\n",
        "plt.scatter(X2[:,1], Y, color='red', marker='+', label='Training Data')\n",
        "plt.plot(X2, X2.dot(theta2), color='green', label='Linear Regression')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(f\"X1 vs Y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, iterations + 1), cost_history2, color='blue')\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost (J)')\n",
        "plt.title('Convergence of gradient descent X2 vs Y')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GmdpMn8GbdU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta3, cost_history3 = gradient_descent(X3, Y, theta0, alpha, iterations)\n",
        "\n",
        "#print('Final value of theta =', theta3)\n",
        "print('cost_history =', cost_history3)\n",
        "\n",
        "plt.scatter(X3[:,1], Y, color='red', marker='+', label='Training Data')\n",
        "plt.plot(X3, X3.dot(theta3), color='green', label='Linear Regression')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(f\"X1 vs Y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, iterations + 1), cost_history3, color='blue')\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost (J)')\n",
        "plt.title('Convergence of gradient descent X3 vs Y')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o5DhYzcWbo53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}